{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pywt\n",
    "from copy import deepcopy\n",
    "from ttknn.light_utility import Utility\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.join(os.getcwd(), '..')\n",
    "if parent_dir not in sys.path: sys.path.append(parent_dir)\n",
    "from torch import nn\n",
    "from mkit.torch_support.tensor_utils import (\n",
    "    sequential_x_y_split, xy_to_tensordataset,\n",
    ")\n",
    "from mkit.torch_support.nn_utils import training_loop\n",
    "import geobleu\n",
    "from IPython.display import clear_output\n",
    "# from model.CNN import FlexibleCNN\n",
    "from model.NN import NN\n",
    "from module.utility import LabelEncoder\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('./cityD-dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_series = df['x'].values\n",
    "ay_series = df['y'].values\n",
    "def wavelet_denoise(data, wavelet='db4', level=None, alpha=1.0):\n",
    "    coeffs = pywt.wavedec(data, wavelet, mode=\"per\", level=level)\n",
    "    sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
    "    # Multiply the universal threshold by \"alpha\" (explained below)\n",
    "    uthresh = alpha * sigma * np.sqrt(2 * np.log(len(data))) \n",
    "    \n",
    "    # Soft-threshold detail coefficients\n",
    "    coeffs[1:] = [pywt.threshold(c, value=uthresh, mode='soft') for c in coeffs[1:]]\n",
    "    \n",
    "    return pywt.waverec(coeffs, wavelet, mode=\"per\")\n",
    "\n",
    "# Example usage\n",
    "ax_denoised = wavelet_denoise(ax_series, wavelet='db4', level=5, alpha=5)[:len(ax_series)]\n",
    "ay_denoised = wavelet_denoise(ay_series, wavelet='db4', level=5, alpha=5)[:len(ay_series)]\n",
    "df['denoised_x'] = np.round(ax_denoised).astype(int)\n",
    "df['denoised_y'] = np.round(ay_denoised).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load .env variables\n",
    "load_dotenv()\n",
    "SPLIT_DATE = 60\n",
    "END_DATE = 75\n",
    "NUM_OF_TIMESTAMPS = 48\n",
    "SAMPLE_NUM = 3_000\n",
    "VOCAB=40401\n",
    "LENGTHS = [NUM_OF_TIMESTAMPS * 7]\n",
    "EPOCHS = 20\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "uid = 35\n",
    "uid_df = df[df.uid == uid]\n",
    "train_df, test_df = uid_df[uid_df.d < 60], uid_df[uid_df.d > 60]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_template = pd.MultiIndex.from_product([train_df.uid.unique(), range(SPLIT_DATE), range(48)])\n",
    "train_template = pd.DataFrame(index=train_template).reset_index()\n",
    "train_template.columns = ['uid', 'd', 't']\n",
    "test_template = pd.DataFrame(index=pd.MultiIndex.from_product([range(SPLIT_DATE, END_DATE), range(NUM_OF_TIMESTAMPS)])).reset_index()\n",
    "test_template.columns = ['d', 't']\n",
    "train_df = train_template.merge(train_df, on=['uid', 'd', 't'], how='left')\n",
    "train_df = train_df.fillna(method='ffill').fillna(method='bfill')\n",
    "'''Inference'''\n",
    "AHEAD = test_template.d.unique().__len__() * test_template.t.unique().__len__() # length of target sequences\n",
    "'''Training Data Processing'''\n",
    "tmp_train_df = train_df[train_df.columns[-2:].values].copy()\n",
    "tmp_train_df = tmp_train_df.rename(columns={'denoised_x': \"x\", \"denoised_y\": 'y'})\n",
    "seq_train = train_df.dropna().apply(encoder.transform, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_model() missing 1 required positional argument: 'look_back'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m         losses\u001b[38;5;241m.\u001b[39mappend(avg_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, final_segment\n\u001b[1;32m---> 51\u001b[0m x_model, x_final_segment \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLENGTHS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m y_model, y_final_segment \u001b[38;5;241m=\u001b[39m get_model(LENGTHS[\u001b[38;5;241m0\u001b[39m], col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: get_model() missing 1 required positional argument: 'look_back'"
     ]
    }
   ],
   "source": [
    "\n",
    "class SingleNN(nn.Module):\n",
    "    def __init__(self, window_size, embed_dim, vocab):\n",
    "        super(SingleNN, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab, embed_dim)\n",
    "        self.window_size = window_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim * window_size, 16),\n",
    "            nn.LayerNorm(16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, vocab),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "def get_model(df, look_back, col='x'):\n",
    "\n",
    "    train_x, train_y = sequential_x_y_split(\n",
    "        df[col].values,\n",
    "        look_back=look_back,\n",
    "    )\n",
    "    train_y = train_y.ravel()\n",
    "    model = SingleNN(train_x.shape[1], 100, VOCAB)\n",
    "    train_loader = xy_to_tensordataset(train_x, train_y, return_loader=True)\n",
    "    model = model.cuda()\n",
    "    final_segment = train_x[-1]\n",
    "    losses = []\n",
    "    optim = torch.optim.Adamax(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(20):\n",
    "        avg_loss = 0.0\n",
    "        for loader in train_loader:\n",
    "            x, y = loader\n",
    "            x = x.cuda().to(torch.int64)\n",
    "            y = y.cuda().to(torch.int64)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            avg_loss += loss.item()\n",
    "        losses.append(avg_loss / len(train_loader))\n",
    "    return model, final_segment\n",
    "\n",
    "x_model, x_final_segment = get_model(LENGTHS[0], col='x')\n",
    "y_model, y_final_segment = get_model(LENGTHS[0], col='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial in_x shape: torch.Size([1, 336])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 720/720 [00:00<00:00, 786.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial in_x shape: torch.Size([1, 336])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 720/720 [00:00<00:00, 743.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "113.20664650155058"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict_sequence(model, initial_tokens, ahead, device):\n",
    "    \"\"\"\n",
    "    Generate 'ahead' tokens from the 'model' given an initial token sequence.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        initial_tokens (list or np.array): The initial token sequence.\n",
    "        ahead (int): Number of tokens to predict.\n",
    "        device (torch.device): The device (CPU or GPU).\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of predicted tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert initial tokens to a tensor of shape (1, sequence_length)\n",
    "    in_x = torch.tensor(initial_tokens, dtype=torch.int64).to(device).unsqueeze(0)\n",
    "    print(\"Initial in_x shape:\", in_x.shape)\n",
    "    \n",
    "    sequences = []\n",
    "    for _ in tqdm(range(ahead)):\n",
    "        # Forward pass to get the model's output\n",
    "        out = model(in_x)\n",
    "        \n",
    "        # Take the index of the most likely token\n",
    "        out_token = torch.argmax(out, dim=1).unsqueeze(0)\n",
    "        in_x = torch.concat([in_x, out_token], dim=1)[:, 1:]\n",
    "        \n",
    "        # Store the predicted token\n",
    "        sequences.append(out_token[0].item())\n",
    "    \n",
    "    return sequences\n",
    "x_segments = predict_sequence(x_model, x_final_segment, AHEAD, DEVICE)\n",
    "y_segments = predict_sequence(y_model, y_final_segment, AHEAD, DEVICE)\n",
    "tmp_template = test_template\n",
    "tmp_template['x'] = x_segments\n",
    "tmp_template['y'] = y_segments\n",
    "tmp_template = test_df[['uid', 'd', 't']].merge(tmp_template, on=['d', 't'], how='left')\n",
    "\n",
    "dtw_score = geobleu.calc_dtw(\n",
    "    Utility.to_eval_format(tmp_template),\n",
    "    Utility.to_eval_format(test_df)\n",
    ")\n",
    "dtw_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154.30421415852805"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[0;32m      2\u001b[0m length \u001b[38;5;241m=\u001b[39m LENGTHS[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m train_x, train_y \u001b[38;5;241m=\u001b[39m sequential_x_y_split(\n\u001b[0;32m      7\u001b[0m     seq_train\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m      8\u001b[0m     look_back\u001b[38;5;241m=\u001b[39mlength,\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise Exception\n",
    "length = LENGTHS[0]\n",
    "\n",
    "\n",
    "\n",
    "train_x, train_y = sequential_x_y_split(\n",
    "    seq_train.values,\n",
    "    look_back=length,\n",
    ")\n",
    "train_y = train_y.ravel()\n",
    "train_x.shape, train_y.shape\n",
    "# 3) Convert to Torch loaders\n",
    "train_loader, val_loader = xy_to_tensordataset(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    shuffle=False,\n",
    "    input_dtype=torch.float,\n",
    "    output_dtype=torch.long,\n",
    "    return_loader=True,\n",
    "    val_ratio=0.2\n",
    ")\n",
    "nn_model = NN(\n",
    "    input_size=length,\n",
    "    start_dim=32,\n",
    "    n_layers=5,\n",
    "    output_size=VOCAB\n",
    ")\n",
    "# 4) Train the model\n",
    "training_loop(\n",
    "    nn_model,\n",
    "    device=DEVICE,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=torch.optim.Adam(nn_model.parameters()),\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    epochs=10,\n",
    "    val_loader=val_loader,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# If you're in a notebook, you might want to clear the output each iteration:\n",
    "# clear_output(wait=True)\n",
    "print(f\"Finished training for length={length}. Now predicting...\")\n",
    "\n",
    "# 5) Model prediction\n",
    "nn_model.eval()\n",
    "nn_model.cpu()\n",
    "\n",
    "sequences = []\n",
    "# Start input from the last training example\n",
    "in_x = torch.tensor(np.array([train_x[-1]]))\n",
    "\n",
    "for _ in tqdm(range(AHEAD)):\n",
    "    # Forward pass\n",
    "    out = torch.argmax(nn_model(in_x.float()), dim=1).unsqueeze(-1)\n",
    "    # Append the newly predicted token\n",
    "    in_x = torch.concat([in_x, out], dim=1)[:, 1:]\n",
    "    sequences.append(out[0])\n",
    "\n",
    "# Decode the entire generated sequence\n",
    "predicted_tokens = torch.concat(sequences).tolist()\n",
    "target_val = [encoder.decode(i) for i in predicted_tokens]\n",
    "\n",
    "# 6) Format the prediction output\n",
    "tmp_df = test_template.copy()\n",
    "tmp_df[['x', 'y']] = np.array(target_val)\n",
    "\n",
    "test_uid_df = test_df[test_df.uid == uid]  # Testing subset for current uid\n",
    "sub_df = test_uid_df[['d', 't']].merge(tmp_df, how='left')\n",
    "\n",
    "# 7) Calculate score (DTW) and store results\n",
    "dtw_score = geobleu.calc_dtw(\n",
    "    Utility.to_eval_format(sub_df),\n",
    "    Utility.to_eval_format(test_uid_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxy(data):\n",
    "    return data.values[:, 0], data.values[:, 1]\n",
    "x, y = toxy(test_uid_df[['x', 'y']])\n",
    "plt.scatter(x, y)\n",
    "x, y = toxy(sub_df[['x', 'y']])\n",
    "plt.scatter(x, y, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxy(data):\n",
    "    return data.values[:, 0], data.values[:, 1]\n",
    "x, y = toxy(test_uid_df[['x', 'y']])\n",
    "plt.scatter(x, y)\n",
    "x, y = toxy(sub_df[['x', 'y']])\n",
    "plt.scatter(x, y, s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []  # List to store outcomes for each length\n",
    "\n",
    "for length in LENGTHS:\n",
    "    # 1) Define the NN model\n",
    "    nn_model = NN(\n",
    "        input_size=length,\n",
    "        start_dim=32,\n",
    "        n_layers=5,\n",
    "        output_size=VOCAB\n",
    "    )\n",
    "\n",
    "    # 2) Prepare the training data\n",
    "    train_x, train_y = sequential_x_y_split(\n",
    "        seq_train.values,\n",
    "        look_back=length,\n",
    "    )\n",
    "    train_y = train_y.ravel()\n",
    "\n",
    "    # 3) Convert to Torch loaders\n",
    "    train_loader, val_loader = xy_to_tensordataset(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        shuffle=False,\n",
    "        input_dtype=torch.float,\n",
    "        output_dtype=torch.long,\n",
    "        return_loader=True,\n",
    "        val_ratio=0.2\n",
    "    )\n",
    "\n",
    "    # 4) Train the model\n",
    "    training_loop(\n",
    "        nn_model,\n",
    "        device=DEVICE,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=torch.optim.Adam(nn_model.parameters()),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        epochs=EPOCHS,\n",
    "        val_loader=val_loader,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # If you're in a notebook, you might want to clear the output each iteration:\n",
    "    # clear_output(wait=True)\n",
    "    print(f\"Finished training for length={length}. Now predicting...\")\n",
    "\n",
    "    # 5) Model prediction\n",
    "    nn_model.eval()\n",
    "    nn_model.cpu()\n",
    "\n",
    "    sequences = []\n",
    "    # Start input from the last training example\n",
    "    in_x = torch.tensor(np.array([train_x[-1]]))\n",
    "\n",
    "    for _ in tqdm(range(AHEAD)):\n",
    "        # Forward pass\n",
    "        out = torch.argmax(nn_model(in_x.float()), dim=1).unsqueeze(-1)\n",
    "        # Append the newly predicted token\n",
    "        in_x = torch.concat([in_x, out], dim=1)[:, 1:]\n",
    "        sequences.append(out[0])\n",
    "\n",
    "    # Decode the entire generated sequence\n",
    "    predicted_tokens = torch.concat(sequences).tolist()\n",
    "    target_val = [encoder.decode(i) for i in predicted_tokens]\n",
    "\n",
    "    # 6) Format the prediction output\n",
    "    tmp_df = test_template.copy()\n",
    "    tmp_df[['x', 'y']] = np.array(target_val)\n",
    "\n",
    "    test_uid_df = test_df[test_df.uid == uid]  # Testing subset for current uid\n",
    "    sub_df = test_uid_df[['d', 't']].merge(tmp_df, how='left')\n",
    "\n",
    "    # 7) Calculate score (DTW) and store results\n",
    "    dtw_score = geobleu.calc_dtw(\n",
    "        Utility.to_eval_format(sub_df),\n",
    "        Utility.to_eval_format(test_uid_df)\n",
    "    )\n",
    "\n",
    "    # Store everything of interest in a dictionary\n",
    "    iteration_result = {\n",
    "        'length': length,\n",
    "        'predicted_sequence': target_val,\n",
    "        'dtw_score': dtw_score\n",
    "    }\n",
    "    results.append(iteration_result)\n",
    "\n",
    "# After the loop, you can analyze all results:\n",
    "# for r in results:\n",
    "#     print(f\"Length: {r['length']}, DTW: {r['dtw_score']}, Sequence sample: {r['predicted_sequence'][:5]}\")\n",
    "\n",
    "# Or convert to a pandas DataFrame if you like:\n",
    "# import pandas as pd\n",
    "# results_df = pd.DataFrame(results)\n",
    "# display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape, in_x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
